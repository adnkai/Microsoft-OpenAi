using Azure.AI.OpenAI;
using Azure.Core;
using Azure;

using System.Text.Json;
// See https://aka.ms/new-console-template for more information


Uri uri = new Uri("https://openaiservicekr.openai.azure.com/");
var key = "";

var creds = new AzureKeyCredential(key);
var myRequest = "Tell me something about the new world order.";
Console.WriteLine("Ask something!");
myRequest = Console.ReadLine();

var data = new {
    prompt = new[] {
        myRequest
    },
    max_tokens = 420, // max number of tokens. Min 0
    temperature = 0.2, // sampling temperature (0.9 for creative responses, 0 for strict)
    //top_p = 0.2, // nucleus sampling (top 10% = 0.1)
    /*logit_bias = new {
        key = 1234,
        // logit_bias: Dictionary<string, number>, # Optional. Defaults to null. Modify the likelihood of specified tokens appearing in the
        // completion. Accepts a json object that maps tokens (specified by their token ID
        // in the GPT tokenizer) to an associated bias value from -100 to 100. You can use
        // this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
        // token IDs. Mathematically, the bias is added to the logits generated by the
        // model prior to sampling. The exact effect will vary per model, but values
        // between -1 and 1 should decrease or increase likelihood of selection; values
        // like -100 or 100 should result in a ban or exclusive selection of the relevant
        // token. As an example, you can pass {"50256" &amp;#58; -100} to prevent the
        // &lt;|endoftext|&gt; token from being generated.
    },*/
    // user = "<user>", // for tracking
    n = 2, // how many snippets (0-128)
    logprobs = 2, // how many tokens to return (0-100)
    // model = "<model>", // which model to use
    echo = true, // true = echo prompt and compleation
    /*stop = new[] {
        "<String>"
    },*/
    // completion_config = "<completion_config>",
    cache_level = 1,
    presence_penalty = 1, // how likely is repetition
    frequency_penalty = 1, // topic variance
    best_of = 10, // how many variations to create
};

var _client = new OpenAIClient(uri, creds);



var req = RequestContent.Create(data);

Response response = await _client.GetCompletionsAsync("GPT", req);

JsonElement result = JsonDocument.Parse(response.ContentStream!).RootElement;

// Console.WriteLine(result.GetProperty("id").ToString());
// Console.WriteLine(result.GetProperty("object").ToString());
// Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine();
Console.WriteLine("Used model: " + result.GetProperty("model").ToString());
Console.WriteLine();
Console.WriteLine("Response: ");
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("text").ToString());
// Console.WriteLine(result.GetProperty("choices")[0].GetProperty("index").ToString());
// Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("tokens")[0].ToString());

// Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("token_logprobs")[0].ToString());
// Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("top_logprobs")[0].GetProperty("<test>").ToString());

// Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("top_logprobs").ToString());

// Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("text_offset")[0].ToString());
Console.WriteLine();
Console.WriteLine("Finished with reason: " + result.GetProperty("choices")[0].GetProperty("finish_reason").ToString());
/*
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
*/